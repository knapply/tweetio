---
title: "`{tweetio}`"
output:
  github_document:
    html_preview: true
    toc: true
    toc_depth: 2
  html_document:
    keep_md: yes
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

<!-- README.Rmd generates README.md. -->

```{r, echo=FALSE}
knitr::opts_chunk$set(
  # collapse = TRUE,
  fig.align = "center",
  comment = "#>",
  fig.path = "man/figures/",
  message = FALSE,
  warning = FALSE
)

options(width = 150)
```


<!-- badges: start -->
[![Lifecycle](https://img.shields.io/badge/lifecycle-maturing-blue.svg)](https://www.tidyverse.org/lifecycle/#maturing)
[![AppVeyor build status](https://ci.appveyor.com/api/projects/status/github/knapply/tweetio?branch=master&svg=true)](https://ci.appveyor.com/project/knapply/tweetio)
[![Travis-CI Build Status](https://travis-ci.org/knapply/tweetio.svg?branch=master)](https://travis-ci.org/knapply/tweetio)
[![Codecov test coverage](https://codecov.io/gh/knapply/tweetio/branch/master/graph/badge.svg)](https://codecov.io/gh/knapply/tweetio?branch=master)
[![GitHub last commit](https://img.shields.io/github/last-commit/knapply/tweetio.svg)](https://github.com/knapply/tweetio/commits/master)
[![License: GPL v3](https://img.shields.io/badge/License-GPLv3-blue.svg)](https://www.gnu.org/licenses/gpl-3.0)
[![Depends](https://img.shields.io/badge/Depends-GNU_R>=4.0-blue.svg)](https://www.r-project.org/)
<!-- [![HitCount](http://hits.dwyl.io/knapply/tweetio.svg)](http://hits.dwyl.io/knapply/tweetio) -->
<!-- badges: end -->

<!-- [![R build status](https://github.com/knapply/tweetio/workflows/R-CMD-check/badge.svg)](https://github.com/knapply/tweetio/actions?workflow=R-CMD-check) -->






# Introduction

`{tweetio}`'s goal is to enable safe, efficient I/O and transformation of Twitter data. Whether the data came from the Twitter API, a database dump, or some other source, `{tweetio}`'s job is to get them into R and ready for analysis.

`{tweetio}` is __not__ a competitor to [`{rtweet}`](https://rtweet.info/): it is not interested in collecting Twitter data. That said, it definitely attempts to compliment it by emulating its data frame schema because...

1. It's incredibly easy to use.
2. It's more efficient to analyze than a key-value format following the raw data.
3. It'd be a waste not to maximize compatibility with tools built specifically around `{rtweet}`'s data frames.

# Installation

You'll need a C++ compiler. If you're using Windows, that means [Rtools](https://cran.r-project.org/bin/windows/Rtools/).

```{r, eval=FALSE}
if (!requireNamespace("remotes", quietly = TRUE)) install.packages("remotes")

remotes::install_github("knapply/tweetio")
```


# Usage

```{r}
library(tweetio)
```

`{tweetio}` uses [`{data.table}`](https://rdatatable.gitlab.io/data.table/) internally for performance and stability reasons, but if you're a [`{tidyverse}`](https://www.tidyverse.org/) fan who's accustomed to dealing with `tibble`s, you can set an option so that `tibble`s are _always_ returned.

Because `tibble`s have an incredibly informative and user-friendly `print()` method, we'll set the option for examples. Note that if the `{tibble}` package is not installed, this option is ignored.

```{r}
options(tweetio.as_tibble = TRUE)
```

You can check on all available `{tweetio}` options using `tweetio_options()`.

```{r}
tweetio_options()
```




<!-- # What's New? -->

<!-- ## Easy Access to Twitter-disclosed Information Operations Archives -->

<!-- ```{r} -->
<!-- io_campaign_metadata -->
<!-- ``` -->


## Simple Example

First, we'll save a stream of tweets using `rtweet::stream_tweets()`.

```{r}
temp_file <- tempfile(fileext = ".json")
rtweet::stream_tweets(timeout = 15, parse = FALSE,
                      file_name = temp_file)
```

We can then pass the file path to `tweetio::read_tweets()` to efficiently parse the data into an `{rtweet}`-style data frame.

```{r}
tiny_rtweet_stream <- read_tweets(temp_file)
tiny_rtweet_stream
```


## Performance

`rtweet::parse_stream()` is totally sufficient for smaller files (as long as the returned data are valid JSON), but `tweetio::read_tweets()` is _much_ faster.

```{r}
small_rtweet_stream <- "inst/example-data/api-stream-small.json.gz"

res <- bench::mark(
  rtweet = rtweet::parse_stream(small_rtweet_stream),
  tweetio = tweetio::read_tweets(small_rtweet_stream)
  ,
  check = FALSE,
  filter_gc = FALSE,
  relative = TRUE
)

res[, 1:9]
```

With bigger files, using `rtweet::parse_stream()` is no longer realistic, especially if the JSON are invalid, but _big_ tweet data sets are where `{tweetio}` can help.

```{r}
many_files <- dir("inst/example-data/", full.names = TRUE, recursive = TRUE)
scales::number_bytes(sum(file.size(many_files)))

big_df <- read_tweets(many_files)
big_df
```


## Data Dumps

A common practice for handling social media data at scale is to store them in search engine databases like Elasticsearch, but it's (unfortunately) possible that you'll need to work with data dumps.

This has three unfortunate consequences:

1. Routines that were purpose-built to work directly with `{rtweet}`'s data frames can't play along with your data.
2. You're going to waste most of your time (and memory) getting data into R that you're not going to use.
3. The data are _very_ tedious to restructure in R (lists of lists of lists of lists of lists...).

`{tweetio}` solves this by parsing everything and building the data frames at the C++ level, including handling GZIP files for you.


# Spatial Tweets

If you have `{sf}` installed, you can use `as_tweet_sf()` to only keep those tweets that contain valid bounding box polygons or points.

```{r}
tweet_sf <- as_tweet_sf(big_df)
tweet_sf$geometry
```


There are currently four columns that can potentially hold spatial geometries:

1. `"bbox_coords"`
2. `"quoted_bbox_coords"`
3. `"retweet_bbox_coords"`
4. `"geo_coords"`

You can select which one to use to build your `sf` object by modifying the `geom_col=` parameter (default: `"bbox_coords"`)

```{r}
as_tweet_sf(big_df, geom_col = "retweet_bbox_coords")$geometry
```

You can also build _all_ the supported bounding boxes by setting `geom_col=` to `"all"`.

```{r}
all_bboxes <- as_tweet_sf(big_df, geom_col = "all")
all_bboxes[, c("which_geom", "geometry")]
```


From there, you can easily use the data like any other `{sf}` object.


```{r, fig.width=12, fig.height=8}
library(ggplot2)

world <- rnaturalearth::ne_countries(returnclass = "sf")
world <- world[world$continent != "Antarctica", ]

all_bboxes <- all_bboxes[order(sf::st_area(all_bboxes$geometry), decreasing = TRUE), ]

ggplot(all_bboxes) +
  geom_sf(fill = "white", color = "lightgray", data = world) +
  geom_sf(aes(fill = which_geom), color = "black" ,
          alpha = 0.1, size = 0.1, show.legend = TRUE) +
  coord_sf(crs = 3857) +
  scale_fill_viridis_d() +
  scale_color_viridis_d() +
  theme_void() +
  theme(legend.title = element_blank(), legend.position = "top",
        panel.background = element_rect(fill = "#daf3ff"))
```

# Tweet Networks

If you want to analyze tweet networks and have `{igraph}` or `{network}` installed, you can get started immediately using `tweetio::as_tweet_igraph()` or `tweetio::as_tweet_network()`.

```{r}
as_tweet_igraph(big_df)
as_tweet_network(big_df[1:1e4, ])
```

If you want to take advantage of all the metadata available, you can set `all_status_data` and/or `all_user_data` to `TRUE`

```{r}
as_tweet_igraph(big_df,
                all_user_data = TRUE, all_status_data = TRUE)
as_tweet_network(big_df[1:1e4, ],
                 all_user_data = TRUE, all_status_data = TRUE)
```

## Two-Mode Networks

You can also build two-mode networks by specifying the `target_class` as `"hashtag"`s, `"url"`s, or `"media"`. 

* Returned `<igraph>`s will be set as bipartite following `{igraph}`'s convention of a `logical` vertex attribute specifying each partition. Accounts are always `TRUE`.
* Returned `<network>`s will be set as bipartite following `{network}`'s convention of ordering the "actors" first, and setting the network-level attribute of "bipartite" as the number of "actors". Accounts are always the "actors".

If bipartite, the returned objects are always set as undirected.

### Users to Hashtags

```{r}
as_tweet_igraph(big_df, target_class = "hashtag")
as_tweet_network(big_df[1:1e4, ], target_class = "hashtag")
```

### Users to URLs

```{r}
as_tweet_igraph(big_df, target_class = "url")
as_tweet_network(big_df[1:1e4, ], target_class = "url")
```

### Users to Media

```{r}
as_tweet_igraph(big_df, target_class = "media")
as_tweet_network(big_df[1:1e4, ], target_class = "media")
```

## `<proto_net>`

You're not stuck with going directly to `<igraph>`s or `<network>`s though. Underneath the hood, `as_tweet_igraph()` and `as_tweet_network()` use `as_proto_net()` to build a `<proto_net>`, a list of edge and node data frames.

```{r}
as_proto_net(big_df,
             all_status_data = TRUE, all_user_data = TRUE)
```


# Progress

### Supported Data Inputs

- [x] Twitter Streaming API (NDJSON/JSONL)
- [x] Twitter REST API (/statuses)
- [x] API to Elasticsearch-style document arrays (`/_source/doc`)
- [x] API to Elasticsearch-style NDJSON/JSONL (`/doc`)

### Supported Data Outputs

- [x] CSV
- [x] Gephi-friendly GraphML

### Structures

- [x] `{rtweet}`-style data frames
- [x] Spatial Tweets via `{sf}`
- [x] Tweet networks via `{igraph}`
- [x] Tweet networks via `{network}`
